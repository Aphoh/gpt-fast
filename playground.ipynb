{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./checkpoints/bigcode/starcoder2-3b\", torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Transformer\n",
    "from util import load_model\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_path = Path(\"./checkpoints/bigcode/starcoder2-3b/model.pth\")\n",
    "with torch.device(\"meta\"):\n",
    "    model = Transformer.from_name(ckpt_path.parent.name)\n",
    "model = load_model(\n",
    "    model,\n",
    "    \"./checkpoints/bigcode/starcoder2-3b/model.pth\",\n",
    "    device=\"cuda\",\n",
    "    precision=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0304 05:13:15.236000 3519 torch/_inductor/select_algorithm.py:1477] [0/0] Exception out of resource: shared memory, Required: 164864, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. for benchmark choice TritonTemplateCaller(/tmp/torchinductor_will/so/csoeq4w2c5uyyi4vaacy7fpbij4aql4hbyqwit3xmvqkgdlzio3r.py, BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=128, FLOAT32_PRECISION=\"'ieee'\", GQA_SHARED_HEADS=16, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=False, OUTPUT_LOGSUMEXP=False, PRESCALE_QK=False, QK_HEAD_DIM=128, ROWS_GUARANTEED_SAFE=False, SAFE_M_BOUNDARY=False, SAFE_N_BOUNDARY=True, SM_SCALE=0.08838834764831843, SPARSE_KV_BLOCK_SIZE=128, SPLIT_KV=64, V_HEAD_DIM=128, num_stages=3, num_warps=2)\n",
      "W0304 05:13:15.616000 3519 torch/_inductor/select_algorithm.py:1696] [0/0] out of resource: shared memory, Required: 164864, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.\n",
      "AUTOTUNE flex_decoding(1x2x16x5x128, 1x2x4096x128, 1x2x4096x128, 1x64x32x5, 1x64x32x5, 1x1x1, 1x1x1x32, 1x1x1, 1x1x1x32, )\n",
      "  triton_flex_decoding_1 0.1454 ms 100.0% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=32, FLOAT32_PRECISION=\"'ieee'\", GQA_SHARED_HEADS=16, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=False, OUTPUT_LOGSUMEXP=False, PRESCALE_QK=False, QK_HEAD_DIM=128, ROWS_GUARANTEED_SAFE=False, SAFE_M_BOUNDARY=False, SAFE_N_BOUNDARY=True, SM_SCALE=0.08838834764831843, SPARSE_KV_BLOCK_SIZE=128, SPLIT_KV=64, V_HEAD_DIM=128, num_stages=3, num_warps=2\n",
      "  triton_flex_decoding_0 0.5827 ms 25.0% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=64, FLOAT32_PRECISION=\"'ieee'\", GQA_SHARED_HEADS=16, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=False, OUTPUT_LOGSUMEXP=False, PRESCALE_QK=False, QK_HEAD_DIM=128, ROWS_GUARANTEED_SAFE=False, SAFE_M_BOUNDARY=False, SAFE_N_BOUNDARY=True, SM_SCALE=0.08838834764831843, SPARSE_KV_BLOCK_SIZE=128, SPLIT_KV=64, V_HEAD_DIM=128, num_stages=1, num_warps=2\n",
      "  triton_flex_decoding_2 inf ms 0.0% BLOCKS_ARE_CONTIGUOUS=False, BLOCK_M=128, BLOCK_N=128, FLOAT32_PRECISION=\"'ieee'\", GQA_SHARED_HEADS=16, HAS_FULL_BLOCKS=True, IS_DIVISIBLE=False, OUTPUT_LOGSUMEXP=False, PRESCALE_QK=False, QK_HEAD_DIM=128, ROWS_GUARANTEED_SAFE=False, SAFE_M_BOUNDARY=False, SAFE_N_BOUNDARY=True, SM_SCALE=0.08838834764831843, SPARSE_KV_BLOCK_SIZE=128, SPLIT_KV=64, V_HEAD_DIM=128, num_stages=3, num_warps=2\n",
      "SingleProcess AUTOTUNE benchmarking takes 0.3799 seconds and 25.1276 seconds precompiling for 3 choices\n",
      "/tmp/ipykernel_3519/340634684.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.topk(torch.nn.functional.softmax(logits[0, 0]), 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.0554, 0.0488, 0.0459, 0.0231, 0.0192, 0.0149], device='cuda:0',\n",
       "       dtype=torch.bfloat16),\n",
       "indices=tensor([ 913,  640, 2594, 1176,  100,  634], device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.attention.flex_attention import create_block_mask\n",
    "\n",
    "\n",
    "def causal_mask(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "\n",
    "def prefill(\n",
    "    model: Transformer, x: torch.Tensor, input_pos: torch.Tensor, **sampling_kwargs\n",
    ") -> torch.Tensor:\n",
    "    # input_pos: [B, S]\n",
    "    mask = create_block_mask(\n",
    "        causal_mask, 1, 1, input_pos.shape[0], model.max_seq_length, device=x.device\n",
    "    )\n",
    "    logits = model(mask, x, input_pos)\n",
    "    return logits\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "with device:\n",
    "    with torch.inference_mode():\n",
    "        input_ids = tokenizer.encode(\"def foo(x):\")\n",
    "        input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "        input_pos = torch.arange(input_ids.shape[1])\n",
    "        model.setup_caches(1, 4096)\n",
    "        logits = prefill(model, input_ids, input_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3519/241237498.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.topk(torch.nn.functional.softmax(logits[0, 0]), 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.0554, 0.0488, 0.0459, 0.0231, 0.0192, 0.0149], device='cuda:0',\n",
       "       dtype=torch.bfloat16),\n",
       "indices=tensor([ 913,  640, 2594, 1176,  100,  634], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(torch.nn.functional.softmax(logits[0, 0]), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3519/3314666126.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.topk(torch.nn.functional.softmax(hf_logits[0, 0]), 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.0562, 0.0466, 0.0439, 0.0234, 0.0208, 0.0151], device='cuda:0',\n",
       "       dtype=torch.bfloat16),\n",
       "indices=tensor([ 913,  640, 2594, 1176,  100,  634], device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    hf_logits = hf_model(input_ids).logits\n",
    "torch.topk(torch.nn.functional.softmax(hf_logits[0, 0]), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
